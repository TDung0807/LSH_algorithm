{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requiment 1"
      ],
      "metadata": {
        "id": "6h3u_GQOVaIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "uPqyY43fVi3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import hashlib\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time"
      ],
      "metadata": {
        "id": "lL4pxe1GVlqF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "4jfP_eUIlgOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "      with open(filename, 'r') as file:\n",
        "          text = file.readlines()\n",
        "      df = pd.DataFrame(text, columns=[\"Text\"])\n",
        "      return df"
      ],
      "metadata": {
        "id": "IHm9o58CqpPS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSH"
      ],
      "metadata": {
        "id": "3sNgM1e1lVph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 1:"
      ],
      "metadata": {
        "id": "AzGQDW3_1p4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InMemoryMinHashLSH:\n",
        "\n",
        "    def __init__(self, documents):\n",
        "      self.documents = documents\n",
        "    pass\n",
        "\n",
        "    def convert_to_shingling(self, text, k):\n",
        "      shingles = set()\n",
        "      words = text.split(\" \")\n",
        "      for i in range(len(words) - k + 1):\n",
        "          tmp = \" \".join(words[i:i+k])\n",
        "          shingles.add(tmp)\n",
        "      return shingles\n",
        "\n",
        "    def convert_to_boolean_vector(self, df, text):\n",
        "      union_shingles = set()  # Initialize as a set\n",
        "      for _, row in df.iterrows():\n",
        "          union_shingles |= set(row['Shingles'])  # Use set union operation\n",
        "      # Convert the set to a numpy array for efficient membership checking\n",
        "      union = np.array(list(union_shingles))\n",
        "      result = []\n",
        "      for shingle in union:\n",
        "          if shingle in text:\n",
        "              result.append(1)\n",
        "          else:\n",
        "              result.append(0)\n",
        "      return result\n",
        "\n",
        "\n",
        "    def shingling(self,df):\n",
        "      shingles_list = []\n",
        "      boolean_list = []\n",
        "\n",
        "      for index, row in df.iterrows():\n",
        "          text = row['Text'].replace(',',\"\").replace(';',\"\").replace('.',\"\").replace('\\'',\"\")\n",
        "          shingles = self.convert_to_shingling(text, k=8)  # Adjust k value as needed\n",
        "          shingles_list.append(shingles)\n",
        "      shingles_df = pd.DataFrame({'Shingles': shingles_list})\n",
        "\n",
        "      for index, row in df.iterrows():\n",
        "          text = row['Text'].replace(',',\"\").replace(';',\"\").replace('.',\"\").replace('\\'',\"\")\n",
        "          boolean_vector = self.convert_to_boolean_vector(shingles_df,text)  # Adjust k value as needed\n",
        "          boolean_list.append(boolean_vector)\n",
        "      boolean_vector_df = pd.DataFrame({'vector': boolean_list})\n",
        "      return boolean_vector_df\n",
        "\n",
        "    def convert_to_signature(self,text):\n",
        "      vector = np.array(text)\n",
        "      arr = self.generate_arrHash(len(vector))\n",
        "      result=[]\n",
        "      for i in range(1,50):\n",
        "        random.shuffle(arr)\n",
        "        for j in range(1,len(arr)):\n",
        "          if vector[arr.index(j)]==1:\n",
        "            result.append(arr.index(j))\n",
        "            break\n",
        "      return np.array(result)\n",
        "\n",
        "    def minhashing(self,df):\n",
        "      signature_list=[]\n",
        "      for index, row in df.iterrows():\n",
        "        text = row['vector']\n",
        "        minhash = self.convert_to_signature(text)\n",
        "        signature_list.append(minhash)\n",
        "      signature_df = pd.DataFrame({'signature': signature_list})\n",
        "\n",
        "      return signature_df\n",
        "\n",
        "    def generate_arrHash(self,n):\n",
        "      arr = [i for i in range(1, n+1)]\n",
        "      return arr\n",
        "\n",
        "    def split_signature(self, signature, k):\n",
        "        result = []\n",
        "        for i in range(0, len(signature), k):\n",
        "            tmp = signature[i:i+k].tolist()\n",
        "            result.append(tmp)\n",
        "        return result\n",
        "    # def hash_busket(self,signature_split):\n",
        "\n",
        "\n",
        "    def truncated_hash(self, vector, length):\n",
        "        hash_value = hashlib.sha256(bytes(str(vector), 'utf-8')).hexdigest()\n",
        "        truncated_value = int(hash_value[:length], 16)\n",
        "        result = {truncated_value: vector}\n",
        "        return result\n",
        "\n",
        "    def locality_sensitivity_hashing(self, signature):\n",
        "        result = {}\n",
        "        for index, row in signature.iterrows():\n",
        "            text = row['signature']\n",
        "            for vector in self.split_signature(text, 2):\n",
        "                truncated_hash_result = self.truncated_hash(vector, 4)\n",
        "                for key, value in truncated_hash_result.items():\n",
        "                    if key not in result:\n",
        "                        result[key] = []\n",
        "                    result[key].append(value)\n",
        "            # break\n",
        "        return result\n",
        "\n",
        "    def check_bucket(self,bucket):\n",
        "      for i in bucket.values():\n",
        "        if len(i) > 1:\n",
        "            print(i)\n",
        "\n",
        "    def run(self):\n",
        "      shingling_result= self.shingling(self.documents)\n",
        "      # sig = self.minhashing(shingling_result)\n",
        "      # result = self.locality_sensitivity_hashing(sig)\n",
        "      # self.check_bucket(result)\n",
        "      return\n",
        "\n",
        "\n",
        "    def approxNearestNeighbors(self, key, n):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "-37WdYFdmzWK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 2:"
      ],
      "metadata": {
        "id": "w6u7OY5u1thS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InMemoryMinHashLSH:\n",
        "\n",
        "    def __init__(self, documents):\n",
        "      self.documents = documents\n",
        "    pass\n",
        "\n",
        "    def convert_to_shingling(self, text, k):\n",
        "      shingles = set()\n",
        "      words = text.split(\" \")\n",
        "      for i in range(len(words) - k + 1):\n",
        "          tmp = \" \".join(words[i:i+k])\n",
        "          shingles.add(tmp)\n",
        "      return shingles\n",
        "\n",
        "\n",
        "    def convert_to_boolean_vector(self, df, text):\n",
        "      union_shingles = set().union(*df['Shingles'])\n",
        "      union = np.array(list(union_shingles))\n",
        "      result = np.zeros(len(union), dtype=int) # Initialize result as a NumPy array of zeros\n",
        "      for i, shingle in enumerate(union):\n",
        "          if shingle in text:\n",
        "              result[i] = 1\n",
        "      return result\n",
        "\n",
        "\n",
        "    def shingling(self, df):\n",
        "      def preprocess_text(text):\n",
        "          return text.replace(',', \"\").replace(';', \"\").replace('.', \"\").replace('\\'', \"\")\n",
        "      df['Text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "      shingles_df = pd.DataFrame({'Shingles': df['Text'].apply(lambda text: self.convert_to_shingling(text, k=8))})\n",
        "      boolean_vector_df = pd.DataFrame({'vector': df['Text'].apply(lambda text: self.convert_to_boolean_vector(shingles_df, text))})\n",
        "\n",
        "      return boolean_vector_df\n",
        "\n",
        "\n",
        "    def convert_to_signature(self, text):\n",
        "      vector = np.array(text)\n",
        "      arr = self.generate_arr_hash(len(vector)-1)\n",
        "      result_list = []  # Use a list for intermediate storage\n",
        "      arr_set = set(arr)  # Convert arr to a set for faster lookup\n",
        "\n",
        "      for i in range(1, 50):\n",
        "          np.random.shuffle(arr)  # Shuffle the array using numpy\n",
        "          for j in range(len(arr)):\n",
        "            if vector[arr[j]] == 1:\n",
        "                result_list.append(arr[j])  # Append to the list\n",
        "                break\n",
        "      return np.array(result_list)  # Convert the list to a numpy array\n",
        "\n",
        "    def minhashing(self,df):\n",
        "      signature_list=[]\n",
        "      for index, row in df.iterrows():\n",
        "        text = row['vector']\n",
        "        minhash = self.convert_to_signature(text)\n",
        "        signature_list.append(minhash)\n",
        "      signature_df = pd.DataFrame({'signature': signature_list})\n",
        "\n",
        "      return signature_df\n",
        "\n",
        "    def generate_arr_hash(self,n):\n",
        "      arr = [i for i in range(1, n+1)]\n",
        "      return arr\n",
        "\n",
        "    def split_signature(self, signature, k):\n",
        "        result = []\n",
        "        for i in range(0, len(signature), k):\n",
        "            tmp = signature[i:i+k].tolist()\n",
        "            result.append(tmp)\n",
        "        return result\n",
        "    # def hash_busket(self,signature_split):\n",
        "\n",
        "\n",
        "    def truncated_hash(self, vector, length):\n",
        "        hash_value = hashlib.sha256(bytes(str(vector), 'utf-8')).hexdigest()\n",
        "        truncated_value = int(hash_value[:length], 16)\n",
        "        result = {truncated_value: vector}\n",
        "        return result\n",
        "\n",
        "    def locality_sensitivity_hashing(self, signature):\n",
        "      result = {}\n",
        "\n",
        "      def process_row(row):\n",
        "          text = row['signature']\n",
        "          hash_results = {}  # Change to a dictionary\n",
        "          for vector in self.split_signature(text, 2):\n",
        "              truncated_hash_result = self.truncated_hash(vector, 4)\n",
        "              for key, value in truncated_hash_result.items():\n",
        "                  hash_results.setdefault(key, []).extend(value)\n",
        "          return hash_results\n",
        "\n",
        "      # Apply the process_row function to each row of the DataFrame\n",
        "      hash_results_list = signature.apply(process_row, axis=1)\n",
        "\n",
        "      # Aggregate the hash results into the final result dictionary\n",
        "      for hash_results in hash_results_list:\n",
        "          for key, value in hash_results.items():\n",
        "              result.setdefault(key, []).extend(value)\n",
        "\n",
        "      return result\n",
        "\n",
        "    def check_bucket(self,bucket):\n",
        "      for i in bucket.values():\n",
        "        if len(i) > 3:\n",
        "            print(i)\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "      ###Shingling\n",
        "      start_time = time.time()\n",
        "      shingling_result= self.shingling(self.documents)\n",
        "      end_time = time.time()\n",
        "      print(\"part 1(shingling): \", end_time - start_time)\n",
        "\n",
        "      ###Minhashing\n",
        "      start_time = time.time()\n",
        "      sig = self.minhashing(shingling_result)\n",
        "      end_time = time.time()\n",
        "      print(\"part 2(minhashing): \", end_time - start_time)\n",
        "\n",
        "\n",
        "      ###LSH\n",
        "      start_time = time.time()\n",
        "      result = self.locality_sensitivity_hashing(sig)\n",
        "      end_time = time.time()\n",
        "      print(\"part 3(LSH): \", end_time - start_time)\n",
        "\n",
        "      return\n",
        "\n",
        "\n",
        "    def approxNearestNeighbors(self, key, n):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "_C-3O0TN1wZq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running"
      ],
      "metadata": {
        "id": "nMdUSTHs1ya2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_file(\"/content/WebOfScience-5736.txt\")\n",
        "lsh = InMemoryMinHashLSH(data)\n",
        "lsh.run()"
      ],
      "metadata": {
        "id": "vP3-kXcmnNx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8921fefc-bdf8-4c16-e437-d97f7c11544e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part 1(shingling):  2694.1030468940735\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uPqyY43fVi3_",
        "4jfP_eUIlgOe",
        "AzGQDW3_1p4W"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}