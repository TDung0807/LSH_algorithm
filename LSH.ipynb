{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requiment 1"
      ],
      "metadata": {
        "id": "6h3u_GQOVaIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "uPqyY43fVi3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import hashlib\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "lL4pxe1GVlqF"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "4jfP_eUIlgOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "      with open(filename, 'r') as file:\n",
        "          text = file.readlines()\n",
        "      df = pd.DataFrame(text, columns=[\"Text\"])\n",
        "      return df"
      ],
      "metadata": {
        "id": "IHm9o58CqpPS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSH"
      ],
      "metadata": {
        "id": "3sNgM1e1lVph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InMemoryMinHashLSH:\n",
        "\n",
        "    def __init__(self, documents):\n",
        "      self.documents = documents\n",
        "    pass\n",
        "\n",
        "    def convert_to_shingling(self, text, k):\n",
        "      shingles = set()\n",
        "      words = text.split(\" \")\n",
        "      for i in range(len(words) - k + 1):\n",
        "          tmp = \" \".join(words[i:i+k])\n",
        "          shingles.add(tmp)\n",
        "      return shingles\n",
        "\n",
        "    def convert_to_boolean_vector(self, df, text):\n",
        "      union_shingles = set()  # Initialize as a set\n",
        "      for _, row in df.iterrows():\n",
        "          union_shingles |= set(row['Shingles'])  # Use set union operation\n",
        "      # Convert the set to a numpy array for efficient membership checking\n",
        "      union = np.array(list(union_shingles))\n",
        "      result = []\n",
        "      for shingle in union:\n",
        "          if shingle in text:\n",
        "              result.append(1)\n",
        "          else:\n",
        "              result.append(0)\n",
        "      return result\n",
        "\n",
        "\n",
        "    def shingling(self,df):\n",
        "      shingles_list = []\n",
        "      for index, row in df.iterrows():\n",
        "          text = row['Text'].replace(',',\"\").replace(';',\"\").replace('.',\"\").replace('\\'',\"\")\n",
        "          shingles = self.convert_to_shingling(text, k=10)  # Adjust k value as needed\n",
        "          shingles_list.append(shingles)\n",
        "      shingles_df = pd.DataFrame({'Shingles': shingles_list})\n",
        "\n",
        "      boolean_list = []\n",
        "      for index, row in df.iterrows():\n",
        "          text = row['Text'].replace(',',\"\").replace(';',\"\").replace('.',\"\").replace('\\'',\"\")\n",
        "          boolean_vector = self.convert_to_boolean_vector(shingles_df,text)  # Adjust k value as needed\n",
        "          boolean_list.append(boolean_vector)\n",
        "      boolean_vector_df = pd.DataFrame({'vector': boolean_list})\n",
        "      self.boolean_vector_df = boolean_vector_df\n",
        "      pass\n",
        "\n",
        "    def convert_to_signature(self,text):\n",
        "      vector = np.array(text)\n",
        "      arr = self.generate_arrHash(len(vector))\n",
        "      result=[]\n",
        "      for i in range(1,50):\n",
        "        random.shuffle(arr)\n",
        "        for j in range(1,len(arr)):\n",
        "          if vector[arr.index(j)]==1:\n",
        "            result.append(arr.index(j))\n",
        "            break\n",
        "      return np.array(result)\n",
        "\n",
        "    def minhashing(self,df):\n",
        "      signature_list=[]\n",
        "      for index, row in df.iterrows():\n",
        "        text = row['vector']\n",
        "        minhash = self.convert_to_signature(text)\n",
        "        signature_list.append(minhash)\n",
        "      signature_df = pd.DataFrame({'signature': signature_list})\n",
        "\n",
        "      return signature_df\n",
        "\n",
        "\n",
        "      ###### Test Function\n",
        "    def calculate_similarity(self,vector1, vector2):\n",
        "      vector1 = np.array(vector1).reshape(1, -1)\n",
        "      vector2 = np.array(vector2).reshape(1, -1)\n",
        "      return cosine_similarity(vector1, vector2)[0][0]\n",
        "\n",
        "      ############ End Test Function\n",
        "\n",
        "    def generate_arrHash(self,n):\n",
        "      arr = [i for i in range(1, n+1)]\n",
        "      return arr\n",
        "\n",
        "    def locality_sensity_hashing(self):\n",
        "        # Your locality sensitive hashing implementation here\n",
        "        pass\n",
        "\n",
        "    def run(self):\n",
        "      self.shingling(self.documents)\n",
        "      self.minhashing(self.boolean_vector_df)\n",
        "      return\n",
        "\n",
        "\n",
        "    def approxNearestNeighbors(self, key, n):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "-37WdYFdmzWK"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_file(\"/content/WebOfScience-5736.txt\")\n",
        "lsh = InMemoryMinHashLSH(data)\n",
        "lsh.run()"
      ],
      "metadata": {
        "id": "vP3-kXcmnNx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4a7f3e-9ab5-49f3-8ce5-fe3633c093d4"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Row1  Row2  Similarity\n",
            "0       0     1         0.0\n",
            "1       0     2         0.0\n",
            "2       0     3         0.0\n",
            "3       0     4         0.0\n",
            "4       0     5         0.0\n",
            "..    ...   ...         ...\n",
            "166    15    17         0.0\n",
            "167    15    18         0.0\n",
            "168    16    17         0.0\n",
            "169    16    18         0.0\n",
            "170    17    18         0.0\n",
            "\n",
            "[171 rows x 3 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}