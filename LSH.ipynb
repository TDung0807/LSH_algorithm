{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h3u_GQOVaIp"
      },
      "source": [
        "# Requiment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPqyY43fVi3_"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lL4pxe1GVlqF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jfP_eUIlgOe"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IHm9o58CqpPS"
      },
      "outputs": [],
      "source": [
        "def read_file(filename):\n",
        "      with open(filename, 'r') as file:\n",
        "          text = file.readlines()\n",
        "      df = pd.DataFrame(text, columns=[\"Text\"])\n",
        "      return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sNgM1e1lVph"
      },
      "source": [
        "## LSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzGQDW3_1p4W"
      },
      "source": [
        "### Version 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-37WdYFdmzWK"
      },
      "outputs": [],
      "source": [
        "class InMemoryMinHashLSH:\n",
        "\n",
        "    def __init__(self, documents):\n",
        "      self.documents = documents\n",
        "    pass\n",
        "\n",
        "    def convert_to_shingling(self, text, k):\n",
        "      shingles = set()\n",
        "      words = text.split(\" \")\n",
        "      for i in range(len(words) - k + 1):\n",
        "          tmp = \" \".join(words[i:i+k])\n",
        "          shingles.add(tmp)\n",
        "      return shingles\n",
        "\n",
        "    def convert_to_boolean_vector(self, df, text):\n",
        "      union_shingles = set()  # Initialize as a set\n",
        "      for _, row in df.iterrows():\n",
        "          union_shingles |= set(row['Shingles'])  # Use set union operation\n",
        "      # Convert the set to a numpy array for efficient membership checking\n",
        "      union = np.array(list(union_shingles))\n",
        "      result = []\n",
        "      for shingle in union:\n",
        "          if shingle in text:\n",
        "              result.append(1)\n",
        "          else:\n",
        "              result.append(0)\n",
        "      return result\n",
        "\n",
        "\n",
        "    def shingling(self,df):\n",
        "      shingles_list = []\n",
        "      boolean_list = []\n",
        "\n",
        "      for index, row in df.iterrows():\n",
        "          text = row['Text'].replace(',',\"\").replace(';',\"\").replace('.',\"\").replace('\\'',\"\")\n",
        "          shingles = self.convert_to_shingling(text, k=8)  # Adjust k value as needed\n",
        "          shingles_list.append(shingles)\n",
        "      shingles_df = pd.DataFrame({'Shingles': shingles_list})\n",
        "\n",
        "      for index, row in df.iterrows():\n",
        "          text = row['Text'].replace(',',\"\").replace(';',\"\").replace('.',\"\").replace('\\'',\"\")\n",
        "          boolean_vector = self.convert_to_boolean_vector(shingles_df,text)  # Adjust k value as needed\n",
        "          boolean_list.append(boolean_vector)\n",
        "      boolean_vector_df = pd.DataFrame({'vector': boolean_list})\n",
        "      return boolean_vector_df\n",
        "\n",
        "    def convert_to_signature(self,text,arr):\n",
        "      vector = np.array(text)\n",
        "      result=[]\n",
        "      for i in range(1,50):\n",
        "        random.shuffle(arr)\n",
        "        for j in range(1,len(arr)):\n",
        "          if vector[arr.index(j)]==1:\n",
        "            result.append(arr.index(j))\n",
        "            break\n",
        "      return np.array(result)\n",
        "\n",
        "    def minhashing(self,df):\n",
        "      signature_list=[]\n",
        "      for index, row in df.iterrows():\n",
        "        arr = self.generate_arrHash(len(text))\n",
        "        text = row['vector']\n",
        "        minhash = self.convert_to_signature(text,arr)\n",
        "        signature_list.append(minhash)\n",
        "      signature_df = pd.DataFrame({'signature': signature_list})\n",
        "\n",
        "      return signature_df\n",
        "\n",
        "    def generate_arrHash(self,n):\n",
        "      arr = [i for i in range(1, n+1)]\n",
        "      return arr\n",
        "\n",
        "    def split_signature(self, signature, k):\n",
        "        result = []\n",
        "        for i in range(0, len(signature), k):\n",
        "            tmp = signature[i:i+k].tolist()\n",
        "            result.append(tmp)\n",
        "        return result\n",
        "    # def hash_busket(self,signature_split):\n",
        "\n",
        "\n",
        "    def truncated_hash(self, vector, length):\n",
        "        hash_value = hashlib.sha256(bytes(str(vector), 'utf-8')).hexdigest()\n",
        "        truncated_value = int(hash_value[:length], 16)\n",
        "        result = {truncated_value: vector}\n",
        "        return result\n",
        "\n",
        "    def locality_sensitivity_hashing(self, signature):\n",
        "        result = {}\n",
        "        for index, row in signature.iterrows():\n",
        "            text = row['signature']\n",
        "            for vector in self.split_signature(text, 2):\n",
        "                truncated_hash_result = self.truncated_hash(vector, 4)\n",
        "                for key, value in truncated_hash_result.items():\n",
        "                    if key not in result:\n",
        "                        result[key] = []\n",
        "                    result[key].append(value)\n",
        "            # break\n",
        "        return result\n",
        "\n",
        "    def check_bucket(self,bucket):\n",
        "      for i in bucket.values():\n",
        "        if len(i) > 1:\n",
        "            print(i)\n",
        "\n",
        "    def run(self):\n",
        "      shingling_result= self.shingling(self.documents)\n",
        "\n",
        "      return\n",
        "\n",
        "\n",
        "    def approxNearestNeighbors(self, key, n):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6u7OY5u1thS"
      },
      "source": [
        "### Version 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C-3O0TN1wZq"
      },
      "outputs": [],
      "source": [
        "class InMemoryMinHashLSH:\n",
        "\n",
        "    def __init__(self, documents):\n",
        "      self.documents = documents\n",
        "    pass\n",
        "\n",
        "    def shingling(self, df):\n",
        "      def preprocess_text(text):\n",
        "          text = text.replace(',', \"\").replace(';', \"\").replace('.', \"\").replace('(', \"\").replace(')', \"\")\n",
        "          return text.lower()\n",
        "      df['Text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "      def convert_to_shingling(text, k):\n",
        "        shingles = set()\n",
        "        words = text.split(\" \")\n",
        "\n",
        "        # Compute the hash for the first k-gram\n",
        "        current_hash = hash(\" \".join(words[:k]))\n",
        "        shingles.add(current_hash)\n",
        "\n",
        "        # Use rolling hash to efficiently compute hashes for subsequent k-grams\n",
        "        base = 101  # Choose a suitable base for the rolling hash\n",
        "        power = base ** (k - 1)\n",
        "\n",
        "        for i in range(1, len(words) - k + 1):\n",
        "            # Remove the contribution of the first word from the previous k-gram\n",
        "            current_hash -= ord(words[i - 1][0]) * power  # Assuming we only take the first character\n",
        "            # Shift the hash by multiplying base and add the hash of the new word\n",
        "            current_hash = current_hash * base + ord(words[i + k - 1][0])  # Assuming we only take the first character\n",
        "            shingles.add(current_hash)\n",
        "\n",
        "        return shingles\n",
        "\n",
        "      def convert_to_boolean(text, union):\n",
        "        result = []\n",
        "        for shingle in union:\n",
        "            if shingle in text:\n",
        "                result.append(1)\n",
        "            else:\n",
        "                result.append(0)\n",
        "        return np.array(result)\n",
        "      def process1(df, k):\n",
        "        df['Text'] = df['Text'].apply(lambda text: convert_to_shingling(text, k))\n",
        "\n",
        "      process1(df,8)\n",
        "\n",
        "      def process2(df):\n",
        "          all_shingles = set()\n",
        "          for text in df['Text']:\n",
        "            all_shingles.update(text)\n",
        "          print(len(all_shingles))\n",
        "          df['vector']=df['Text'].apply(lambda text:convert_to_boolean(text,all_shingles))\n",
        "          return df\n",
        "\n",
        "      process2(df)\n",
        "\n",
        "      return df\n",
        "\n",
        "\n",
        "\n",
        "    def minhashing(self,df):\n",
        "      def permute_row(row, permutation):\n",
        "          return np.array(row)[permutation]\n",
        "\n",
        "      def permute_dataframe(df, permutation):\n",
        "          return pd.DataFrame(df['vector'].apply(lambda row: permute_row(row, permutation)))\n",
        "      k = 50\n",
        "      signatures = []\n",
        "      for i in range(k):\n",
        "          permutation = np.random.permutation(len(df.iloc[0]['vector']))\n",
        "\n",
        "          result = permute_dataframe(df, permutation)\n",
        "          signature = []\n",
        "          for index, row in result.iterrows():\n",
        "              first_one_index = np.where(row['vector'] == 1)[0][0]\n",
        "              signature.append(first_one_index)\n",
        "          signatures.append(signature)\n",
        "\n",
        "      #Create df['signature']\n",
        "      #Tranpose signatures then add to signature_df['signature'], signature_df just have 1 column is signature\n",
        "      signature_array = np.array(signatures).T\n",
        "      # Create DataFrame with column name 'signature'\n",
        "      signature_df = pd.DataFrame({'signature': signature_array.tolist()})\n",
        "\n",
        "      return signature_df\n",
        "\n",
        "    def locality_sensitivity_hashing(self, signature):\n",
        "      def split_signature(signature, k):\n",
        "        result = []\n",
        "        for i in range(0, len(signature), k):\n",
        "            tmp = signature[i:i+k]  # Remove .tolist()\n",
        "            result.append(tmp)\n",
        "        return result\n",
        "\n",
        "      def truncated_hash( vector, length):\n",
        "        hash_value = hashlib.sha256(bytes(str(vector), 'utf-8')).hexdigest()\n",
        "        truncated_value = int(hash_value[:length], 16)\n",
        "        result = {truncated_value: vector}\n",
        "        return result\n",
        "\n",
        "      result = {}\n",
        "\n",
        "      def process_row(row):\n",
        "          text = row['signature']\n",
        "          hash_results = {}  # Change to a dictionary\n",
        "          for vector in split_signature(text,2):\n",
        "              truncated_hash_result = truncated_hash(vector, 8)\n",
        "              for key, value in truncated_hash_result.items():\n",
        "                 hash_results.setdefault(key, []).extend((value, row.name))\n",
        "          return hash_results\n",
        "      # Apply the process_row function to each row of the DataFrame\n",
        "      hash_results_list = signature.apply(process_row, axis=1)\n",
        "      # Aggregate the hash results into the final result dictionary\n",
        "      for hash_results in hash_results_list:\n",
        "          for key, value in hash_results.items():\n",
        "              result.setdefault(key, []).extend(value)\n",
        "      return result\n",
        "\n",
        "    def check_bucket(self, bucket, n):\n",
        "      pair_counts = {}  # Dictionary to store counts of pairs of row_index\n",
        "      for key, values in bucket.items():\n",
        "          if len(values) > 2:\n",
        "              # Collect pairs of row_index values\n",
        "              row_indexes = values[1::2]\n",
        "              num_indexes = len(row_indexes)\n",
        "              for i in range(0, num_indexes, 2):  # Iterate over pairs of row_index\n",
        "                  if i + 1 < num_indexes:  # Ensure not to go out of range\n",
        "                      row_index1 = row_indexes[i]\n",
        "                      row_index2 = row_indexes[i+1]\n",
        "                      # Skip pairs where both row indexes are the same\n",
        "                      if row_index1 != row_index2:\n",
        "                          pair = (row_index1, row_index2)\n",
        "                          # Update the count for the current pair\n",
        "                          pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
        "                          vector = values[i]\n",
        "\n",
        "      # Sort pair_counts by count in descending order\n",
        "      sorted_pair_counts = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      # Print n pairs in descending order by count\n",
        "      for i, (pair, count) in enumerate(sorted_pair_counts[:n]):\n",
        "          print(f\"  {pair[0]} and {pair[1]} appear together {count} times.\")\n",
        "          if i == n - 1:\n",
        "              break\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "      ###Shingling\n",
        "      start_time = time.time()\n",
        "      shingling_result= self.shingling(self.documents)\n",
        "      end_time = time.time()\n",
        "      print(\"part 1(shingling): \", end_time - start_time)\n",
        "      ##Minhashing\n",
        "\n",
        "      start_time = time.time()\n",
        "      sig = self.minhashing(shingling_result)\n",
        "      end_time = time.time()\n",
        "      print(\"part 2(minhashing): \", end_time - start_time)\n",
        "      ###LSH\n",
        "      start_time = time.time()\n",
        "      result = self.locality_sensitivity_hashing(sig)\n",
        "      end_time = time.time()\n",
        "      print(\"part 3(LSH): \", end_time - start_time)\n",
        "\n",
        "      self.check_bucket(result,3)\n",
        "      return\n",
        "\n",
        "\n",
        "    def approxNearestNeighbors(self, key, n):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z5jho8Xo-e8"
      },
      "source": [
        "### Version 3:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InMemoryMinHashLSH:\n",
        "\n",
        "    def __init__(self, documents):\n",
        "      self.documents = documents\n",
        "      self.union_shingle = set()\n",
        "      self.permutation = []\n",
        "    pass\n",
        "    def shingling(self, df):\n",
        "      def preprocess_text(text):\n",
        "          text = text.replace(',', \"\").replace(';', \"\").replace('.', \"\").replace('(', \"\").replace(')', \"\")\n",
        "          return text.lower()\n",
        "      df['Text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "      def convert_to_shingling(text, k):\n",
        "        shingles = set()\n",
        "        words = text.split(\" \")\n",
        "        for i in range(len(words) - k + 1):\n",
        "            shingles.add(\" \".join(words[i:i+k]))\n",
        "\n",
        "        return shingles\n",
        "      def convert_to_boolean(text, union):\n",
        "        result = []\n",
        "        for shingle in union:\n",
        "            if shingle in text:\n",
        "                result.append(1)\n",
        "            else:\n",
        "                result.append(0)\n",
        "        return np.array(result)\n",
        "      def process1(df, k):\n",
        "        df['Text'] = df['Text'].apply(lambda text: convert_to_shingling(text, k))\n",
        "\n",
        "      process1(df,1)\n",
        "\n",
        "      def process2(df):\n",
        "          all_shingles = set()\n",
        "          if len(self.union_shingle) != 0:\n",
        "            df['vector']=df['Text'].apply(lambda text:convert_to_boolean(text,self.union_shingle))\n",
        "          else:\n",
        "            for text in df['Text']:\n",
        "              all_shingles.update(text)\n",
        "            print(\"Union length: \", len(all_shingles))\n",
        "            self.union_shingle = all_shingles\n",
        "            df['vector']=df['Text'].apply(lambda text:convert_to_boolean(text,all_shingles))\n",
        "          return df\n",
        "\n",
        "      process2(df)\n",
        "\n",
        "      return df\n",
        "\n",
        "\n",
        "\n",
        "    def minhashing(self, df, k=50):\n",
        "      # Function to permute a row\n",
        "      def permute_row(row, permutation):\n",
        "          return np.array(row)[permutation]\n",
        "      # Generate k permutations and compute signatures\n",
        "      signatures = []\n",
        "      num_shingles = len(df.iloc[0]['vector'])\n",
        "      if len(self.permutation)!=0:\n",
        "        for i in range(len(self.permutation)):\n",
        "            permutation = self.permutation[i]\n",
        "            signature = df['vector'].apply(lambda row: np.where(permute_row(row, permutation) == 1)[0][0]).tolist()\n",
        "            signatures.append(signature)\n",
        "        signature_array = np.array(signatures).T\n",
        "        signature_df = pd.DataFrame({'signature': signature_array.tolist()})\n",
        "      else:\n",
        "        for _ in range(k):\n",
        "            permutation = np.random.permutation(num_shingles)\n",
        "            self.permutation.append(permutation)\n",
        "            signature = df['vector'].apply(lambda row: np.where(permute_row(row, permutation) == 1)[0][0]).tolist()\n",
        "            signatures.append(signature)\n",
        "        signature_array = np.array(signatures).T\n",
        "        signature_df = pd.DataFrame({'signature': signature_array.tolist()})\n",
        "\n",
        "      return signature_df\n",
        "\n",
        "    def locality_sensitivity_hashing(self, signature):\n",
        "      def split_signature(signature, k):\n",
        "        result = []\n",
        "        for i in range(0, len(signature), k):\n",
        "            tmp = signature[i:i+k]  # Remove .tolist()\n",
        "            result.append(tmp)\n",
        "        return result\n",
        "\n",
        "      def truncated_hash(vector, length):\n",
        "        hash_value = hashlib.sha256(bytes(str(vector), 'utf-8')).hexdigest()\n",
        "        truncated_value = int(hash_value[:length], 16)\n",
        "        return truncated_value\n",
        "\n",
        "      result = {}\n",
        "\n",
        "      def process_row(row):\n",
        "        text = row['signature']\n",
        "        hash_results = []\n",
        "        for vector in split_signature(text, 1):\n",
        "            truncated_hash_result = truncated_hash(vector, 8)\n",
        "            hash_results.append(truncated_hash_result)\n",
        "        return hash_results\n",
        "\n",
        "      result_df = pd.DataFrame()\n",
        "      result_df['bucket'] = signature.apply(process_row, axis=1)\n",
        "      return result_df\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "      shingling_result= self.shingling(self.documents)\n",
        "      sig = self.minhashing(shingling_result)\n",
        "      result = self.locality_sensitivity_hashing(sig)\n",
        "      self.bucket = result\n",
        "\n",
        "      return\n",
        "\n",
        "    def approxNearestNeighbors(self, key, n):\n",
        "      shingling_key = self.shingling(key)\n",
        "      sig_key = self.minhashing(shingling_key)\n",
        "      result_key = self.locality_sensitivity_hashing(sig_key)\n",
        "      hash_key = result_key.loc[0,'bucket']\n",
        "      ## Calculate similarity of hash_key each row in self.bucket\n",
        "      def calculate_similarity(signature1, signature2):\n",
        "        set1 = set(signature1)\n",
        "        set2 = set(signature2)\n",
        "        intersection = len(set1.intersection(set2))\n",
        "        union = len(set1.union(set2))\n",
        "        if union == 0:\n",
        "            return 0  # Return 0 if union is 0 to avoid division by zero\n",
        "        else:\n",
        "            similarity = intersection / union\n",
        "            return similarity\n",
        "      result = []\n",
        "      for index, row in self.bucket.iterrows():\n",
        "        value = calculate_similarity(hash_key,row['bucket'])\n",
        "        result.append(value)\n",
        "      self.result_df = pd.DataFrame({'Similarity': result})\n",
        "      self.result_df = self.result_df.sort_values(by='Similarity', ascending=False)\n",
        "\n",
        "      return self.result_df.head(n)\n"
      ],
      "metadata": {
        "id": "fUTfF2z-ywLk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMdUSTHs1ya2"
      },
      "source": [
        "### Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vP3-kXcmnNx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddf8971-b9c0-4092-ec19-277470164d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Union length:  63061\n",
            "time run:  211.91343808174133\n",
            "      Similarity\n",
            "0       0.426471\n",
            "4       0.269231\n",
            "5636    0.195122\n",
            "826     0.195122\n",
            "1353    0.192771\n"
          ]
        }
      ],
      "source": [
        "data = read_file(\"/content/WebOfScience-5736.txt\")\n",
        "key = read_file(\"/content/key.txt\")\n",
        "start = time.time()\n",
        "lsh = InMemoryMinHashLSH(data)\n",
        "lsh.run()\n",
        "end = time.time()\n",
        "print(\"time run: \", end - start)\n",
        "result = lsh.approxNearestNeighbors(key,5)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_file(\"/content/WebOfScience-5736.txt\")\n",
        "key = read_file(\"/content/key.txt\")\n",
        "print(data.loc[4,'Text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hE1KZF_o_eF",
        "outputId": "93ce1489-398b-4c80-c050-209fd575d943"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This paper presents the simulation results of a linear, fully integrated, two-stage digitally programmable 130 nm CMOS power amplifier (PA) operating at 2.4 GHz. Its power stage is composed of a set of amplifying cells which can be enabled or disabled independently by a digital control circuit. All seven operational modes are univocal in terms of 1 dB output compression point (OCP1dB), saturated output power (P-SAT) and power gain at 2.4 GHz. The lowest power mode achieves an 8.1 dBm P-SAT, a 13.5 dB power gain and consumes 171 mW DC power (P-DC) at an OCP1dB of 6 dBm, whereas the highest power mode reaches an 18.9 dBm P-SAT and a 21.1 dB power gain and consumes 415 mW P-DC at an OCP1dB of 18.2 dBm.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uPqyY43fVi3_",
        "4jfP_eUIlgOe",
        "AzGQDW3_1p4W",
        "w6u7OY5u1thS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}